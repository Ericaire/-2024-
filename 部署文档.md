
---
# Sends Training Project 文档

## 项目结构

```
SendsTraningProject/
│
├── flask/                  # Flask应用文件
│   ├── templates/          # 包含home.html与layout.html
│   ├── app.py              # 程序
│   ├── backup.sh           # 用于备份数据的脚本
│   └── dockerfile          
│
├── scrapper/               # 爬虫
│   ├── scrapper.py         # 爬虫程序 
│   └── dockerfile
│
├── nginx/                  # Nginx相关文件   
│   
└── docker-compose.yml
```
## 任务实现

### 爬虫容器
使用基于python3.8镜像的容器，将工作目录设置为/app，并将当前目录内文件全部移动到/app中。通过运行 `pip install --no-cache-dir -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/` 安装依赖。
之前下载出现网络问题，所以这里选择指定阿里云镜像源。
创建并确保用于存放数据库文件的路径 `/var/lib/sqlite` 存在，没注意就报错了。
同时，我们在这里声明容器暴露80端口。

```
EXPOSE 80 # 这样声明没有办法主动开放80端口，但是是一个好的习惯
```

我的爬虫使用了 BeautifulSoup 和 requests 库，数据库使用了 sqlite。运行 `scrapper()` 函数从华侨大学新闻页面爬取内容，处理响应内容，并将新闻的标题和链接存入 sqlite 数据库。
```
from bs4 import BeautifulSoup
import requests from flask_sqlalchemy
import SQLAlchemy from flask import Flask
```
其中SQLAlchemy是Python的SQL工具包
```从华大新闻官网爬取内容
with app.app_context():
    db.create_all()

def scrapper():
    #上下文
    with app.app_context():
        # 这个scrapper() 函数首先使用 requests 库向给华侨大学新闻页面发起请求
        url = "https://news.hqu.edu.cn/hdyw.htm"
        response = requests.get(url)
        html_content = response.content.decode('utf-8')
        # 解码返回的内容，并使用 BeautifulSoup 将内容解析为 lxml 格式
        soup = BeautifulSoup(html_content, 'lxml')
        news_list = soup.find_all('a')
        # 遍历所有的新闻链接（a 标签），获取新闻的标题和链接
        for news in news_list:
            title = news.text
            link = news['href']
            link = 'https://news.hqu.edu.cn/' + link
            existing_news = NewsData.query.filter_by(link=link).first()
                # 检查这条新闻是否已经存在于数据库中
            if existing_news is None:
                # 如果不存在，就创建新的 NewsData 对象
                news_data = NewsData(title=title, link=link)
                # 将其添加到数据会话（db.session）中。
                db.session.add(news_data)
                # 需要注意的是这并不会立即将数据保存到数据库，要实际保存这些对象到数据库需要调用 db.session.commit()
        db.session.commit()

if __name__ == "__main__":
    scrapper()
```

### Flask web应用容器
Flask 定义：基于Python类定义的Web应用框架。与爬虫应用的初始化操作相同。在服务器配置中使用自签名证书。这里选择了Nginx作为反向代理，详情参见 `nginx/` 下的 `hdxw.conf`。
首先在dockerfile中用RUN指令更新安装器并安装一些工具:
```用到这些工具的原因之后会进行说明
RUN apt-get update && apt-get install -y unzip sqlite3 cron nano
```
最后就是使用CMD命令启动flask应用
在Flask web应用的入口文件,这里是'app.py',中
``` 首先定义/hdxw的路由，渲染网页

@app.route('/hdxw')
def home():
        
    all_news_data = NewsData.query.all()
    return render_template('home.html', news_data=all_news_data)

```
home.html就是主页文件;这里把数据库中的数据传递给了news_data, 这些数据在home.html中使用

...

## 其他任务

### 对新闻数据 5min 进行一次更新：
设置定时任务，用Cron和Docker配合使用，从主机上执行 cron 任务 `*/5 * * * * docker run --rm sendstrainingproject-scrapper`。

### 定时导出数据，用Rclone上传腾讯云对象存储
...
---

希望这些建议对你有所帮助！如果你还有其他的问题或需要进一步的帮助，请随时告诉我。
