
全项目的结构：
```
SendsTraningProject/
│
├── flask/ # Flask应用文件
│   └── templates/ # 包含home.html与layout.html
|   └── app.py # 程序
|   └── backup.sh # 用于备份数据的脚本
|   └── dockerfile    
│
├── scrapper/ # 爬虫
|   └── scrapper.py # 爬虫程序 
|   └── dockerfile
|
├── nginx/    # Nginx相关文件
│   
└── docker-compose.yml    
```
 **任务实现**

- 爬虫容器
这里我选择让容器基于python3.8镜像 
将工作目录设置成/app并将当前目录内文件都移动到/app中

```安装一些依赖
RUN pip install --no-cache-dir -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/
```之前用官方源下载遇到问题，这里选择指定镜像源
```注意得先确保指定用来存放数据库文件的路径存在，没注意就报错了
RUN mkdir -p /var/lib/sqlite
```
```声明容器暴露80端口
EXPOSE 80
```这是一个好习惯

这里选择使用BeautifulSoup和requests库
数据库使用sqlite
引入的packages：
```
from bs4 import BeautifulSoup
import requests
from flask_sqlalchemy import SQLAlchemy
from flask import Flask
```
其中SQLAlchemy是Python的SQL工具包

```从华大新闻官网爬取内容
with app.app_context():
    db.create_all()

def scrapper():
    #上下文
    with app.app_context():
        # 这个scrapper() 函数首先使用 requests 库向给华侨大学新闻页面发起请求
        url = "https://news.hqu.edu.cn/hdyw.htm"
        response = requests.get(url)
        html_content = response.content.decode('utf-8')
        # 解码返回的内容，并使用 BeautifulSoup 将内容解析为 lxml 格式
        soup = BeautifulSoup(html_content, 'lxml')
        news_list = soup.find_all('a')
        # 遍历所有的新闻链接（a 标签），获取新闻的标题和链接
        for news in news_list:
            title = news.text
            link = news['href']
            link = 'https://news.hqu.edu.cn/' + link
            existing_news = NewsData.query.filter_by(link=link).first()
                # 检查这条新闻是否已经存在于数据库中
            if existing_news is None:
                # 如果不存在，就创建新的 NewsData 对象
                news_data = NewsData(title=title, link=link)
                # 将其添加到数据会话（db.session）中。
                db.session.add(news_data)
                # 需要注意的是这并不会立即将数据保存到数据库，要实际保存这些对象到数据库需要调用 db.session.commit()
        db.session.commit()

if __name__ == "__main__":
    scrapper()
``` 

- Flask web应用容器
Flask 定义: 通过Python类定义的Web应用框架
具体操作:

更新安装器并安装一些工具:
```用到这些工具的原因之后会进行说明
RUN apt-get update && apt-get install -y unzip sqlite3 cron nano
``

最后就是使用CMD命令启动flask应用

关于应用程序运行的解释：
初始化操作与爬虫应用相同

``` 定义/hdxw的路由，渲染网页

@app.route('/hdxw')
def home():
        
    all_news_data = NewsData.query.all()
    return render_template('home.html', news_data=all_news_data)

```home.html就是主页文件;这里把数据库中的数据传递给了news_data, 这些数据在home.html中使用

如果项目比较大、运用相同的html布局比较多的话，可以考虑将重复的代码单独构成一个layout.html, 方便重复使用
我这里templates分为home.html和layout.html两个文件
```layout.html
<!DOCTYPE html>
<html>
<head>
    <title>华大新闻</title>
</head>
<body>
    {% block body %} {% endblock %}
</body>
</html>

```
```home.html
{% extends 'layout.html' %}

{% block body %}

<h1>华大新闻</h1>
{% for news in news_data %}
<p>
    <a href="{{ news.link }}">{{ news.title }}</a>
</p>
{% endfor %}

{% endblock %}
```
运用了到Jinja来构建和扩展代码块

- Nginx容器
这里选择了Nginx作为反向代理，
在sites-enabled下的hdxw.conf进行配置：
```在server块中;
listen 443 ssl
```设置监听443端口，部署https加密访问功能需要用到

然后这里因为我申请的域名审核还没通过，所以先用自签名证书；操作步骤：
1.创建私钥和公钥
```
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem
```执行之后生成cert.pem和key.pem两个文件(私钥和公钥或者说是'自签名证书')
2.在服务器配置中使用
```然后在这里指定cert.pem和key.pem的路径：
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
```
由于我在docker-compose.yml将nginx文件链接到nginx容器的/etc/nginx
```
     - ./nginx:/etc/nginx
```
所以网页的根目录选择在nginx容器的/etc/nginx/nginx_index:
```
    location / {
        root /etc/nginx/nginx_index;
    }
```
反向代理，可以将用户来自https://116.62.34.42/hdxw的请求转发到http://116.62.34.42:5000/hdxw：
```
    location / {
        proxy_pass http://116.62.34.42:5000;
    }
```

- 容器编排 Docker compose
 由于要设置爬虫5分钟自动运行一次，所以我们希望爬虫容器不会因为程序运行后结束就重启或关闭；
 restart设置为no  

其他任务：

- 对新闻数据5min进行一次更新：
需要设置定时任务
用Cron与Docker配合使用；
```主机上在编辑cron任务
*/5 * * * * docker run --rm sendstrainingproject-scrapper         
```注意这里的操作不是重启容器docker restart，因为执行docker restart后容器会继续运行并保持原有的状态，不会执行新的命令
这样5min启动一次爬虫容器就能自动更新爬取的数据了

- 定时导出数据，用Rclone上传腾讯云对象存储
在flask容器中安装了Rclone工具和
腾讯云s3存储桶
[./]

- `Github Actions`，实现在push改动后自动ssh登录服务器部署最新服务

 **问题解决**  

- 注册了ericxu.fun域名并绑定服务器访问ip, 但icp备案一直没通过

- Git的用法一开始搞错了，后面重做:
  
- docker-compose.yml中nginx容器用到的文件较多，可以考虑直接讲原nginx文件夹链接到容器中，而不是每个文件都单独进行链接
  
- 上下文
